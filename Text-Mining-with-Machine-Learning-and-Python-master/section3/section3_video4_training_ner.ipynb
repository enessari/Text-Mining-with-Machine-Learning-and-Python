{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import random\n",
    "\n",
    "import spacy\n",
    "from spacy.gold import GoldParse\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus.reader.wordnet import NOUN, VERB, ADJ\n",
    "\n",
    "import pycrfsuite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's detect natural disasters!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>through</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>London</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>protest</td>\n",
       "      <td>VB</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #           Word  POS    Tag\n",
       "0  Sentence: 1      Thousands  NNS      O\n",
       "1          NaN             of   IN      O\n",
       "2          NaN  demonstrators  NNS      O\n",
       "3          NaN           have  VBP      O\n",
       "4          NaN        marched  VBN      O\n",
       "5          NaN        through   IN      O\n",
       "6          NaN         London  NNP  B-geo\n",
       "7          NaN             to   TO      O\n",
       "8          NaN        protest   VB      O\n",
       "9          NaN            the   DT      O"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dataset = pd.read_csv(\"ner_dataset.csv\", encoding=\"latin1\")\n",
    "df_dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "last_sent_id= 0\n",
    "for i, row in df_dataset.iterrows():  \n",
    "    if not pd.isnull(row[\"Sentence #\"]):\n",
    "        last_sent_id = int(row[\"Sentence #\"][10:])\n",
    "        row[\"Sentence #\"] = last_sent_id\n",
    "    else:\n",
    "        row[\"Sentence #\"] = last_sent_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find those with 'nat' tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentence #    object\n",
       "Word          object\n",
       "POS           object\n",
       "Tag           object\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dataset.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent_id = df_dataset[df_dataset[\"Tag\"].str.contains(\"nat\")][\"Sentence #\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_dataset_nat = df_dataset[df_dataset[\"Sentence #\"].isin(sent_id)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remap tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lst_tags = df_dataset_nat[\"Tag\"].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lst_tags.remove(\"I-nat\")\n",
    "lst_tags.remove(\"B-nat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dict_tags = {}\n",
    "for i in lst_tags:\n",
    "    dict_tags[i] = \"O\"\n",
    "    \n",
    "dict_tags[\"I-nat\"] = \"I-NAT\"\n",
    "dict_tags[\"B-nat\"] = \"B-NAT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\peter\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "df_dataset_nat[\"Tag remapped\"] = df_dataset_nat[\"Tag\"].map(dict_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'B-NAT', 'I-NAT']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dataset_nat[\"Tag remapped\"].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Using Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LABEL = 'NAT'\n",
    "MAX_ITERATIONS = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def join_space(values):\n",
    "    return \" \".join(values).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_sentences_1 = df_dataset_nat.groupby(\"Sentence #\")[\"Word\"].apply(list).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_sentences_2 = df_dataset_nat.groupby(\"Sentence #\")[\"Tag remapped\"].apply(list).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_sentences = pd.merge(left=df_sentences_1, right = df_sentences_2, on = \"Sentence #\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>Tag remapped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>121</td>\n",
       "      <td>[Officials, say, the, 27-year, old, man, from,...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>206</td>\n",
       "      <td>[Humans, are, usually, infected, with, bird, f...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>227</td>\n",
       "      <td>[One, of, the, 2008, Olympic, mascots, is, mod...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, B-NAT, I-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>229</td>\n",
       "      <td>[Sam, Beattie, reports, from, Jing, Jing, 's, ...</td>\n",
       "      <td>[O, O, O, O, B-NAT, I-NAT, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentence #                                               Word  \\\n",
       "0         121  [Officials, say, the, 27-year, old, man, from,...   \n",
       "1         206  [Humans, are, usually, infected, with, bird, f...   \n",
       "2         227  [One, of, the, 2008, Olympic, mascots, is, mod...   \n",
       "3         229  [Sam, Beattie, reports, from, Jing, Jing, 's, ...   \n",
       "\n",
       "                                        Tag remapped  \n",
       "0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "1  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "2  [O, O, O, O, O, O, O, O, O, O, O, O, B-NAT, I-...  \n",
       "3       [O, O, O, O, B-NAT, I-NAT, O, O, O, O, O, O]  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sentences.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = []\n",
    "\n",
    "for i, row in df_sentences.iterrows():\n",
    "    raw_sent = \" \".join(row[\"Word\"]).replace(\" ,\", \",\")\n",
    "    \n",
    "    tags = list(zip(row[\"Word\"],row[\"Tag remapped\"]))\n",
    "    advance = 0\n",
    "\n",
    "    new_ents = []\n",
    "\n",
    "    for i in range(len(tags)):\n",
    "        tag = tags[i]\n",
    "\n",
    "        word = tag[0]\n",
    "        ent = tag[1]\n",
    "\n",
    "        ent = ent.replace(\"B-\", \"\")\n",
    "        ent = ent.replace(\"I-\", \"\")\n",
    "        ent = ent.replace(\"L-\", \"\")\n",
    "        ent = ent.replace(\"O-\", \"\")\n",
    "        ent = ent.replace(\"U-\", \"\")\n",
    "\n",
    "        ent_range = [advance, advance + len(word), ent]\n",
    "\n",
    "        advance += len(word)\n",
    "        if i < (len(tags) - 1):\n",
    "            if tags[i + 1][0] != ',':\n",
    "                advance += 1\n",
    "\n",
    "        if not ent_range[2] == \"O\":\n",
    "            new_ents.append(ent_range)\n",
    "\n",
    "    new_ents_merged = []\n",
    "\n",
    "    for j in range(len(new_ents)):\n",
    "        if len(new_ents_merged) == 0:\n",
    "            new_ents_merged.append(new_ents[j])\n",
    "\n",
    "        if new_ents_merged[-1][2] == new_ents[j][2]:\n",
    "            new_ents_merged[-1][1] = new_ents[j][1]\n",
    "        else:\n",
    "            new_ents_merged.append(new_ents[j])\n",
    "\n",
    "    new_ents_merged_tuples = [tuple(item) for item in new_ents_merged]\n",
    "    train_data.append((raw_sent, {\"entities\": new_ents_merged_tuples}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\"Officials say the 27-year old man from Vietnam 's northern Ninh Binh \"\n",
      "  'province died late Thursday and tested positive for the H5N1 strain of bird '\n",
      "  'flu .',\n",
      "  {'entities': [(125, 129, 'NAT')]}),\n",
      " ('Humans are usually infected with bird flu by direct contact with infected '\n",
      "  'poultry, but experts fear the H5N1 virus may mutate into a form easily '\n",
      "  'transmitted between people .',\n",
      "  {'entities': [(104, 108, 'NAT')]})]\n"
     ]
    }
   ],
   "source": [
    "pprint(train_data[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = train_data[155:]\n",
    "train_data = train_data[:155]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if 'ner' not in nlp.pipe_names:\n",
    "    ner = nlp.create_pipe('ner')\n",
    "    nlp.add_pipe(ner)\n",
    "\n",
    "else:\n",
    "    ner = nlp.get_pipe('ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ner.add_label(LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 1151.5631708588069}\n",
      "{'ner': 970.3055948485708}\n",
      "Wall time: 1min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "    optimizer = nlp.begin_training()\n",
    "    for itn in range(2):\n",
    "        random.shuffle(train_data)\n",
    "        losses = {}\n",
    "        for text, annotations in train_data:\n",
    "            nlp.update([text], [annotations], sgd=optimizer, drop=0.35,losses=losses)\n",
    "        print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp.meta['name'] = \"en_core_web_sm_newlabel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp.to_disk(\"models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nlp2 = spacy.load(\"models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_true = []\n",
    "\n",
    "for i, test in enumerate(test_data):\n",
    "    y_true.append([test[0][j[0]:j[1]] for j in test[1][\"entities\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_predict = []\n",
    "for test in test_data:\n",
    "    doc = nlp2(test[0])\n",
    "    y_predict.append([ent.text for ent in doc.ents if ent.label_ == \"NAT\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Rita']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(y_predict, y_true):\n",
    "    correct = 0\n",
    "    for j, val in enumerate(y_predict):\n",
    "        if val == y_true[j]:\n",
    "            correct += 1\n",
    "            \n",
    "    return correct / len(y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(y_predict=y_predict, y_true=y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Using PyCRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data = []\n",
    "for index, row in df_sentences.iterrows():\n",
    "    \n",
    "    train_data_sentence = []\n",
    "    \n",
    "    raw_sent = row[\"Word\"]\n",
    "    tokens = nltk.pos_tag(raw_sent)\n",
    "\n",
    "    for i, val in enumerate(tokens):\n",
    "        train_data_word = []\n",
    "        \n",
    "        word = raw_sent[i]\n",
    "        label = row[\"Tag remapped\"][i]\n",
    "        pos_tag = tokens[i][1]\n",
    "\n",
    "        if pos_tag.startswith(\"N\"):\n",
    "            lemma = lemmatizer.lemmatize(word.lower(), pos=NOUN)\n",
    "        elif pos_tag.startswith(\"V\"):\n",
    "            lemma = lemmatizer.lemmatize(word.lower(), pos=VERB)\n",
    "        elif pos_tag.startswith(\"J\"):\n",
    "            lemma = lemmatizer.lemmatize(word.lower(), pos=ADJ)\n",
    "        else:\n",
    "            lemma = word\n",
    "            \n",
    "        train_data_word.append(word)\n",
    "        train_data_word.append(pos_tag)\n",
    "        train_data_word.append(lemma)\n",
    "        train_data_word.append(label)\n",
    "        \n",
    "        train_data_sentence.append(train_data_word)\n",
    "        \n",
    "    train_data.append(train_data_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word2features(sent, i, embed={}, use_gazetteers=False):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][-3]\n",
    "    lemma = sent[i][-2].lower()\n",
    "    features = [\n",
    "        'bias',\n",
    "        'word.lower=' + word.lower(),\n",
    "        'word[-3:]=' + word[-3:],\n",
    "        'word[-2:]=' + word[-2:],\n",
    "        'word.isupper=%s' % word.isupper(),\n",
    "        'word.istitle=%s' % word.istitle(),\n",
    "        'word.isdigit=%s' % word.isdigit(),\n",
    "        'postag=' + postag,\n",
    "        'postag[:2]=' + postag[:2]\n",
    "    ]\n",
    "    if embed != {}:\n",
    "        features.extend(['word.embed=%s' % embed.get(word, len(embed))])\n",
    "    if use_gazetteers:\n",
    "        features.extend(['word.measures=%s' % str(word.lower() in UNIT_GAZETTEER or lemma in UNIT_GAZETTEER),\n",
    "                        'word.products=%s' % str(word.lower() in PRODUCTS_GAZETTEER or lemma in PRODUCTS_GAZETTEER)])\n",
    "\n",
    "    if i > 0:\n",
    "        word1 = sent[i - 1][0]\n",
    "        postag1 = sent[i - 1][-3]\n",
    "        lemma1 = sent[i - 1][-2].lower()\n",
    "        features.extend([\n",
    "            '-1:word.lower=' + word1.lower(),\n",
    "            '-1:word.istitle=%s' % word1.istitle(),\n",
    "            '-1:word.isupper=%s' % word1.isupper(),\n",
    "            '-1:postag=' + postag1,\n",
    "            '-1:postag[:2]=' + postag1[:2]\n",
    "        ])\n",
    "        if embed != {}:\n",
    "            features.extend(['-1:word.embed=%s' % embed.get(word1, len(embed))])\n",
    "        if use_gazetteers:\n",
    "            features.extend(['-1:word.measures=%s' % str(word1.lower() in UNIT_GAZETTEER or lemma1 in UNIT_GAZETTEER),\n",
    "                            '-1:word.products=%s' % str(word1.lower() in PRODUCTS_GAZETTEER or lemma1 in PRODUCTS_GAZETTEER)])\n",
    "\n",
    "    else:\n",
    "        features.append('BOS')\n",
    "\n",
    "    if i < len(sent) - 1:\n",
    "        word1 = sent[i + 1][0]\n",
    "        postag1 = sent[i + 1][-3]\n",
    "        lemma1 = sent[i + 1][-2].lower()\n",
    "        features.extend([\n",
    "            '+1:word.lower=' + word1.lower(),\n",
    "            '+1:word.istitle=%s' % word1.istitle(),\n",
    "            '+1:word.isupper=%s' % word1.isupper(),\n",
    "            '+1:postag=' + postag1,\n",
    "            '+1:postag[:2]=' + postag1[:2]\n",
    "        ])\n",
    "        if use_gazetteers:\n",
    "            features.extend(['+1:word.measures=%s' % str(word1.lower() in UNIT_GAZETTEER or lemma1 in UNIT_GAZETTEER),\n",
    "                            '+1:word.products=%s' % str(word1.lower() in PRODUCTS_GAZETTEER or lemma1 in PRODUCTS_GAZETTEER)])\n",
    "\n",
    "    else:\n",
    "        features.append('EOS')\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sent2features(sent, embed={}, use_gazetteers=False):\n",
    "\n",
    "    return [word2features(sent, i, embed=embed, use_gazetteers=use_gazetteers) for i in range(len(sent))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_formatted = [sent2features(x) for x in train_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_data = df_sentences[\"Tag remapped\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_test = train_data_formatted[155:]\n",
    "y_test = y_data[155:]\n",
    "\n",
    "x_train = train_data_formatted[:155]\n",
    "y_train = y_data[:155]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(X_train, y_train, model_name):\n",
    "    \"\"\" Trains a CRF on the given training data and saves the model. \"\"\"\n",
    "    print(\"Training\", model_name)\n",
    "    trainer = pycrfsuite.Trainer(verbose=False)\n",
    "\n",
    "    for xseq, yseq in zip(X_train, y_train):\n",
    "        trainer.append(xseq, yseq)\n",
    "\n",
    "    trainer.set_params({\n",
    "        'c1': 0.1,  # coefficient for L1 penalty\n",
    "        'c2': 1e-3,  # coefficient for L2 penalty\n",
    "        'feature.possible_transitions': True\n",
    "    })\n",
    "\n",
    "    trainer.train(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training pycrfmodel.model\n",
      "Wall time: 6.36 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train(x_train, y_train, 'pycrfmodel.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tag(X_test,model_name):\n",
    "    \"\"\" Labels test data with the model saved in model_name. \"\"\"\n",
    "    tagger = pycrfsuite.Tagger()\n",
    "    tagger.open(model_name)\n",
    "\n",
    "    return [tagger.tag(seq) for seq in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-NAT',\n",
       " 'I-NAT',\n",
       " 'I-NAT',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag(x_test, 'pycrfmodel.model')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate(y_predict, y_true, ignore_bio = True):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, y_pred in enumerate(y_predict):\n",
    "        for j, y in enumerate(y_pred):\n",
    "            if ignore_bio:\n",
    "                if y[2:] == y_true[i][j][2:]:\n",
    "                    correct += 1\n",
    "                \n",
    "            else:\n",
    "                if y == y_true[i][j]:\n",
    "                    correct += 1\n",
    "                \n",
    "            \n",
    "        \n",
    "        total += len(y_pred)\n",
    "        \n",
    "    return correct / total\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9840213049267643"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(tag(x_test, 'pycrfmodel.model'), y_test, ignore_bio=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
